{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:15.149841Z",
     "iopub.status.busy": "2025-10-09T15:34:15.149584Z",
     "iopub.status.idle": "2025-10-09T15:34:41.876620Z",
     "shell.execute_reply": "2025-10-09T15:34:41.875983Z",
     "shell.execute_reply.started": "2025-10-09T15:34:15.149817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== IMPORTS ===================\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Transformers & PEFT\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:41.878175Z",
     "iopub.status.busy": "2025-10-09T15:34:41.877708Z",
     "iopub.status.idle": "2025-10-09T15:34:41.884801Z",
     "shell.execute_reply": "2025-10-09T15:34:41.884056Z",
     "shell.execute_reply.started": "2025-10-09T15:34:41.878156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== CONFIG ===================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Paths ---\n",
    "IMG_DIR = \"/kaggle/input/dermnet/train\"\n",
    "TEST_DIR = \"/kaggle/working/test_images\"\n",
    "TEXT_JSON = \"/kaggle/input/text-description/text_final.json\"\n",
    "KB_JSON = \"/kaggle/input/knowledge-base-new/kb_new.json\"\n",
    "TEST_TEXT_JSON = \"/kaggle/input/test-2-final/test2_final.json\"\n",
    "\n",
    "# --- Checkpoints ---\n",
    "DINO_CHECKPOINT_PATH = \"/kaggle/working/dino_finetuned.pth\"\n",
    "MAGEVPRO_CHECKPOINT_PATH = \"/kaggle/working/magevpro_best.pth\"\n",
    "\n",
    "# Add these lines to your CONFIG cell\n",
    "S1_CURVES_PATH = \"/kaggle/working/training_curves_S1.png\"\n",
    "S2_CURVES_PATH = \"/kaggle/working/training_curves_S2.png\"\n",
    "\n",
    "# --- Stage 1: Vision Pre-training ---\n",
    "BATCH_SIZE_S1 = 32\n",
    "EPOCHS_S1 = 20\n",
    "PATIENCE_S1 = 4\n",
    "LEARNING_RATE_S1 = 1e-4\n",
    "WEIGHT_DECAY_S1 = 1e-4\n",
    "TEST_SIZE_S1 = 0.2\n",
    "UNFREEZE_DINO_S1 = 2 # Unfreeze last 2 blocks\n",
    "\n",
    "# --- Stage 2: Multimodal Fine-tuning ---\n",
    "BATCH_SIZE_S2 = 16\n",
    "EPOCHS_S2 = 30\n",
    "PATIENCE_S2 = 5\n",
    "LEARNING_RATE_S2 = 5e-5 # LR for head and text LoRA\n",
    "VISION_LR_S2 = 1e-6       # A smaller LR for the vision backbone\n",
    "WEIGHT_DECAY_S2 = 1e-2\n",
    "TEST_SIZE_S2 = 0.2\n",
    "UNFREEZE_DINO_S2 = 1 # Unfreeze only the last block\n",
    "\n",
    "# --- RAG & Inference ---\n",
    "CONFIDENCE_THRESHOLD = 0.6 # Threshold to trigger RAG\n",
    "NUM_INFERENCE_SAMPLES = 5 # Number of validation samples to demonstrate RAG\n",
    "\n",
    "# --- Class Mappings ---\n",
    "CLASS_MAPPINGS = {\n",
    "    'Acne and Rosacea Photos': 'acne',\n",
    "    'Psoriasis pictures Lichen Planus and related diseases': 'psoriasis',\n",
    "    'Eczema Photos': 'eczema',\n",
    "    'Herpes HPV and other STDs Photos': 'stds',\n",
    "    'Tinea Ringworm Candidiasis and other Fungal Infections': 'fungal',\n",
    "    'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions': 'bcc',\n",
    "    'Seborrheic Keratoses and other Benign Tumors': 'seborrheic_keratosis'\n",
    "}\n",
    "FINAL_CLASSES = list(CLASS_MAPPINGS.values())\n",
    "CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(FINAL_CLASSES)}\n",
    "IDX_TO_CLASS = {idx: cls for cls, idx in CLASS_TO_IDX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:41.885762Z",
     "iopub.status.busy": "2025-10-09T15:34:41.885523Z",
     "iopub.status.idle": "2025-10-09T15:34:41.911775Z",
     "shell.execute_reply": "2025-10-09T15:34:41.911189Z",
     "shell.execute_reply.started": "2025-10-09T15:34:41.885735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== KNOWLEDGE BASE (for RAG) ===================\n",
    "class KnowledgeBase:\n",
    "    def __init__(self, kb_path, tokenizer, text_encoder):\n",
    "        print(\"Initializing Knowledge Base...\")\n",
    "        with open(kb_path, 'r') as f:\n",
    "            kb_data = json.load(f)\n",
    "        self.entries = kb_data['dermatology_knowledge_base']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder.to(DEVICE)\n",
    "        self.text_encoder.eval() # Ensure encoder is in eval mode\n",
    "        self.embeddings = self._index_knowledge_base()\n",
    "        print(f\"Knowledge Base indexed with {len(self.embeddings)} entries.\")\n",
    "\n",
    "    def _index_knowledge_base(self):\n",
    "        \"\"\"Encodes all knowledge base entries into vector embeddings.\"\"\"\n",
    "        embeddings = []\n",
    "        for entry in tqdm(self.entries, desc=\"Indexing KB\"):\n",
    "            # Use a descriptive combination of fields for embedding\n",
    "            description = f\"{entry['disease']}: {entry['lesion_morphology']['key_distinctions']}\"\n",
    "            inputs = self.tokenizer(description, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                embedding = self.text_encoder(**inputs).last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    def retrieve(self, description, top_k=3):\n",
    "        \"\"\"Retrieves top_k most similar entries from the KB.\"\"\"\n",
    "        inputs = self.tokenizer(description, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.text_encoder(**inputs).last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(self.entries[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "    def get_differential_questions(self, description):\n",
    "        \"\"\"Gets unique questions from retrieved entries to differentiate diagnoses.\"\"\"\n",
    "        retrieved = self.retrieve(description)\n",
    "        questions = []\n",
    "        for entry, _ in retrieved:\n",
    "            questions.extend(q[\"question\"] for q in entry.get(\"confounder_questions\", []))\n",
    "        return list(set(questions)) # Return unique questions\n",
    "\n",
    "    def refine_prediction(self, description, answers):\n",
    "        \"\"\"Refines prediction based on answers to confounder questions.\"\"\"\n",
    "        retrieved = self.retrieve(description)\n",
    "        scores = {entry['disease']: sim for entry, sim in retrieved}\n",
    "\n",
    "        for entry, _ in retrieved:\n",
    "            for q in entry.get(\"confounder_questions\", []):\n",
    "                question_text = q[\"question\"]\n",
    "                if question_text in answers:\n",
    "                    user_answer = answers[question_text].lower()\n",
    "                    if user_answer == \"yes\" and q.get(\"yes_supports\") in scores:\n",
    "                        scores[q[\"yes_supports\"]] *= 1.5 # Boost score\n",
    "                    elif user_answer == \"no\" and q.get(\"no_supports\") in scores:\n",
    "                        scores[q[\"no_supports\"]] *= 1.5 # Boost score\n",
    "\n",
    "        if not scores: return None, {}\n",
    "        refined_disease = max(scores, key=scores.get)\n",
    "        return refined_disease, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:41.913439Z",
     "iopub.status.busy": "2025-10-09T15:34:41.913012Z",
     "iopub.status.idle": "2025-10-09T15:34:41.934282Z",
     "shell.execute_reply": "2025-10-09T15:34:41.933599Z",
     "shell.execute_reply.started": "2025-10-09T15:34:41.913422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== DATASETS ===================\n",
    "class VisionDataset(Dataset):\n",
    "    \"\"\"Stage-1: Vision-only dataset for images WITHOUT text descriptions.\"\"\"\n",
    "    def __init__(self, img_dir, json_path_to_exclude, transform):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        with open(json_path_to_exclude, 'r') as f:\n",
    "            images_with_text = set(json.load(f).keys())\n",
    "        print(f\"Excluding {len(images_with_text)} images that have text descriptions.\")\n",
    "\n",
    "        for class_folder, class_name in CLASS_MAPPINGS.items():\n",
    "            label = CLASS_TO_IDX[class_name]\n",
    "            class_path = os.path.join(img_dir, class_folder)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_file in os.listdir(class_path):\n",
    "                    if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        relative_path = os.path.join(class_folder, img_file)\n",
    "                        if relative_path not in images_with_text:\n",
    "                            full_img_path = os.path.join(class_path, img_file)\n",
    "                            self.samples.append((full_img_path, label))\n",
    "        print(f\"Found {len(self.samples)} images for Stage-1 vision pre-training.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        return self.transform(image), label\n",
    "\n",
    "\n",
    "class DermDataset(Dataset):\n",
    "    \"\"\"Stage-2: Multimodal dataset for images WITH text descriptions.\"\"\"\n",
    "    def __init__(self, img_dir, json_path, tokenizer, transform):\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        self.samples = []\n",
    "        for rel_path, meta in self.data.items():\n",
    "            class_name = meta.get(\"class\")\n",
    "            text_desc = meta.get(\"description\", \"\")\n",
    "            if class_name in CLASS_TO_IDX:\n",
    "                label = CLASS_TO_IDX[class_name]\n",
    "                full_path = os.path.join(img_dir, rel_path)\n",
    "                if os.path.exists(full_path):\n",
    "                    self.samples.append((full_path, text_desc, label, rel_path))\n",
    "        print(f\"Found {len(self.samples)} multimodal samples for Stage-2 training.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, text, label, rel_path = self.samples[idx]\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        tokens = self.tokenizer(\n",
    "            text, padding=\"max_length\", truncation=True,\n",
    "            max_length=128, return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens = {k: v.squeeze(0) for k, v in tokens.items()}\n",
    "        return image, tokens, label, text, rel_path\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle multimodal batches.\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if not batch: return None, None, None, None, None\n",
    "    images, texts, labels, descriptions, paths = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.tensor(labels)\n",
    "    batched_texts = {k: torch.stack([d[k] for d in texts]) for k in texts[0]}\n",
    "    return images, batched_texts, labels, descriptions, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:41.935023Z",
     "iopub.status.busy": "2025-10-09T15:34:41.934799Z",
     "iopub.status.idle": "2025-10-09T15:34:41.949724Z",
     "shell.execute_reply": "2025-10-09T15:34:41.949022Z",
     "shell.execute_reply.started": "2025-10-09T15:34:41.935007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== MODELS ===================\n",
    "class DinoVisionClassifier(nn.Module):\n",
    "    \"\"\"Stage-1 Vision-only Model.\"\"\"\n",
    "    def __init__(self, num_classes=len(FINAL_CLASSES), unfreeze_blocks=UNFREEZE_DINO_S1):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14', trust_repo=True)\n",
    "        # Freeze all parameters initially\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze the last N blocks for fine-tuning\n",
    "        if unfreeze_blocks > 0:\n",
    "            for block in self.vision_encoder.blocks[-unfreeze_blocks:]:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = True\n",
    "        self.classifier_head = nn.Linear(384, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vision_encoder(x)\n",
    "        return self.classifier_head(features)\n",
    "\n",
    "\n",
    "class MAGEVPro(nn.Module):\n",
    "    \"\"\"Stage-2 Multimodal Model with FiLM Fusion and LoRA.\"\"\"\n",
    "    def __init__(self, num_classes=len(FINAL_CLASSES), dino_checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        # 1. Initialize Vision Encoder from Stage 1\n",
    "        dino_model = DinoVisionClassifier(num_classes=num_classes)\n",
    "        if dino_checkpoint_path and os.path.exists(dino_checkpoint_path):\n",
    "            print(f\"Loading Stage-1 vision weights from {dino_checkpoint_path}\")\n",
    "            dino_model.load_state_dict(torch.load(dino_checkpoint_path, map_location=DEVICE))\n",
    "        self.vision_encoder = dino_model.vision_encoder\n",
    "        \n",
    "        # Freeze all vision parameters initially\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze a smaller number of blocks for Stage 2\n",
    "        if UNFREEZE_DINO_S2 > 0:\n",
    "            for block in self.vision_encoder.blocks[-UNFREEZE_DINO_S2:]:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # 2. Initialize Text Encoder with LoRA\n",
    "        self.text_encoder = BertModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "        lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"query\", \"value\"], lora_dropout=0.1, bias=\"none\")\n",
    "        self.text_encoder = get_peft_model(self.text_encoder, lora_config)\n",
    "        self.text_encoder.print_trainable_parameters()\n",
    "\n",
    "        # 3. Fusion and Classifier Layers\n",
    "        self.film_gamma = nn.Linear(768, 384)\n",
    "        self.film_beta = nn.Linear(768, 384)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, text_inputs, return_attention=False):\n",
    "        image = image.to(DEVICE)\n",
    "        text_inputs = {k: v.to(DEVICE) for k, v in text_inputs.items()}\n",
    "\n",
    "        vision_feat = self.vision_encoder(image)\n",
    "        text_feat = self.text_encoder(**text_inputs).last_hidden_state[:, 0, :] # Get [CLS] token\n",
    "\n",
    "        gamma = self.film_gamma(text_feat)\n",
    "        beta = self.film_beta(text_feat)\n",
    "        fused_features = gamma * vision_feat + beta\n",
    "        \n",
    "        logits = self.mlp(fused_features)\n",
    "\n",
    "        if return_attention:\n",
    "            # Use the magnitude of gamma as a proxy for attention\n",
    "            attention_scores = torch.abs(gamma).mean(dim=0).cpu().detach().numpy()\n",
    "            return logits, attention_scores\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:41.950684Z",
     "iopub.status.busy": "2025-10-09T15:34:41.950388Z",
     "iopub.status.idle": "2025-10-09T15:34:41.969718Z",
     "shell.execute_reply": "2025-10-09T15:34:41.969083Z",
     "shell.execute_reply.started": "2025-10-09T15:34:41.950665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== PLOTTING & UTILS ===================\n",
    "def plot_conf_matrix(y_true, y_pred, save_path=\"confusion_matrix.png\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(FINAL_CLASSES)))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=FINAL_CLASSES, yticklabels=FINAL_CLASSES)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved to {save_path}\")\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, train_accs, val_accs, save_path=\"training_curves.png\"):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title(\"Loss vs. Epochs\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.title(\"Accuracy vs. Epochs\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Training curves saved to {save_path}\")\n",
    "\n",
    "def plot_attention_map(attention_scores, save_path=\"attention_map.png\"):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    sns.heatmap([attention_scores], cmap=\"viridis\", cbar=True)\n",
    "    plt.title(\"Attention Map (FiLM Gamma Weights)\")\n",
    "    plt.xlabel(\"Vision Feature Dimension\")\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:41.970581Z",
     "iopub.status.busy": "2025-10-09T15:34:41.970330Z",
     "iopub.status.idle": "2025-10-09T15:34:41.989612Z",
     "shell.execute_reply": "2025-10-09T15:34:41.988923Z",
     "shell.execute_reply.started": "2025-10-09T15:34:41.970554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== TRAINING LOOPS ===================\n",
    "def train_vision_model(model, train_loader, val_loader, criterion, optimizer, epochs, patience, ckpt_path):\n",
    "    best_val_acc, patience_ctr = 0.0, 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, y_true, y_pred = 0, [], []\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(outputs.argmax(1).cpu().numpy())\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(y_true, y_pred)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        total_loss, y_true, y_pred = 0, [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(outputs.argmax(1).cpu().numpy())\n",
    "        \n",
    "        val_loss = total_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(y_true, y_pred)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_ctr = 0\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"‚úÖ Best model saved with Val Acc: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "    return history\n",
    "\n",
    "def train_multimodal_model(model, train_loader, val_loader, criterion, optimizer, epochs, patience, ckpt_path):\n",
    "    best_val_acc, patience_ctr = 0.0, 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, y_true, y_pred = 0, [], []\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            images, texts, labels, _, _ = batch\n",
    "            if images is None: continue\n",
    "            \n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(outputs.argmax(1).cpu().numpy())\n",
    "            \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(y_true, y_pred)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss, y_true, y_pred = 0, [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                images, texts, labels, _, _ = batch\n",
    "                if images is None: continue\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images, texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(outputs.argmax(1).cpu().numpy())\n",
    "                \n",
    "        val_loss = total_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(y_true, y_pred)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_ctr = 0\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"‚úÖ Best model saved with Val Acc: {best_val_acc:.4f}\")\n",
    "            plot_conf_matrix(y_true, y_pred, save_path=f\"best_val_conf_matrix_S2.png\")\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:41.990535Z",
     "iopub.status.busy": "2025-10-09T15:34:41.990295Z",
     "iopub.status.idle": "2025-10-09T15:34:42.010795Z",
     "shell.execute_reply": "2025-10-09T15:34:42.010126Z",
     "shell.execute_reply.started": "2025-10-09T15:34:41.990519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== STAGE RUNNERS ===================\n",
    "def run_stage1():\n",
    "    print(\"--- Starting Stage 1: Vision Pre-training ---\")\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)), transforms.RandomCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(), transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), \n",
    "        transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    \n",
    "    full_dataset = VisionDataset(IMG_DIR, TEXT_JSON, transform=train_tf)\n",
    "    full_dataset.transform = val_tf # Temporarily switch to val transforms for splitting\n",
    "    \n",
    "    labels = [s[1] for s in full_dataset.samples]\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE_S1, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(np.zeros(len(labels)), labels))\n",
    "    \n",
    "    train_subset = Subset(full_dataset, train_idx)\n",
    "    # Re-assign the training transforms back to the train_subset's underlying dataset\n",
    "    train_subset.dataset.transform = train_tf\n",
    "    val_subset = Subset(full_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE_S1, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE_S1, shuffle=False)\n",
    "    \n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    class_weights = compute_class_weight('balanced', classes=np.arange(len(FINAL_CLASSES)), y=train_labels)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(DEVICE))\n",
    "    \n",
    "    model = DinoVisionClassifier().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                                  lr=LEARNING_RATE_S1, weight_decay=WEIGHT_DECAY_S1)\n",
    "                                  \n",
    "    history = train_vision_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                 EPOCHS_S1, PATIENCE_S1, DINO_CHECKPOINT_PATH)\n",
    "    \n",
    "    # Use the new path variable here\n",
    "    plot_training_curves(history['train_loss'], history['val_loss'], history['train_acc'], history['val_acc'], S1_CURVES_PATH)\n",
    "    print(\"--- Stage 1 Complete ---\")\n",
    "\n",
    "def run_stage2():\n",
    "    print(\"\\n--- Starting Stage 2: Multimodal Fine-tuning ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "    # Load text metadata\n",
    "    with open(TEXT_JSON, \"r\") as f:\n",
    "        full_text_data = json.load(f)\n",
    "    with open(TEST_TEXT_JSON, \"r\") as f:\n",
    "        test_text_data = json.load(f)\n",
    "\n",
    "    # Exclude test keys from full training JSON\n",
    "    test_keys = set(test_text_data.keys())\n",
    "    filtered_text_data = {k: v for k, v in full_text_data.items() if k not in test_keys}\n",
    "\n",
    "    # Save a temporary filtered JSON (optional, for debugging)\n",
    "    filtered_json_path = \"/kaggle/working/train_text_filtered.json\"\n",
    "    with open(filtered_json_path, \"w\") as f:\n",
    "        json.dump(filtered_text_data, f)\n",
    "\n",
    "    # Define transforms\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)), transforms.RandomCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20), transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "        transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    # Use filtered JSON (train only, no test leakage!)\n",
    "    train_ds = DermDataset(IMG_DIR, filtered_json_path, tokenizer, train_tf)\n",
    "    val_ds   = DermDataset(IMG_DIR, filtered_json_path, tokenizer, val_tf)\n",
    "\n",
    "    labels = [s[2] for s in train_ds.samples]\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE_S2, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "    train_loader = DataLoader(Subset(train_ds, train_idx), batch_size=BATCH_SIZE_S2, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(Subset(val_ds, val_idx), batch_size=BATCH_SIZE_S2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    class_weights = compute_class_weight('balanced', classes=np.arange(len(FINAL_CLASSES)), y=train_labels)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(DEVICE))\n",
    "\n",
    "    model = MAGEVPro(dino_checkpoint_path=DINO_CHECKPOINT_PATH).to(DEVICE)\n",
    "\n",
    "    # Differential learning rates\n",
    "    vision_params = [p for n, p in model.vision_encoder.named_parameters() if p.requires_grad]\n",
    "    text_params = [p for n, p in model.text_encoder.named_parameters() if p.requires_grad]\n",
    "    head_params = list(model.film_gamma.parameters()) + list(model.film_beta.parameters()) + list(model.mlp.parameters())\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': vision_params, 'lr': VISION_LR_S2},\n",
    "        {'params': text_params, 'lr': LEARNING_RATE_S2},\n",
    "        {'params': head_params, 'lr': LEARNING_RATE_S2}\n",
    "    ], weight_decay=WEIGHT_DECAY_S2)\n",
    "\n",
    "    history = train_multimodal_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                                     EPOCHS_S2, PATIENCE_S2, MAGEVPRO_CHECKPOINT_PATH)\n",
    "\n",
    "    # Use the new path variable here\n",
    "    plot_training_curves(history['train_loss'], history['val_loss'], history['train_acc'], history['val_acc'], S2_CURVES_PATH)\n",
    "    print(\"--- Stage 2 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:42.011842Z",
     "iopub.status.busy": "2025-10-09T15:34:42.011612Z",
     "iopub.status.idle": "2025-10-09T15:34:42.029812Z",
     "shell.execute_reply": "2025-10-09T15:34:42.029271Z",
     "shell.execute_reply.started": "2025-10-09T15:34:42.011821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def infer_with_rag(model, kb, image_tensor, description, tokenizer, rel_path):\n",
    "    \"\"\"\n",
    "    Performs inference, triggering a RAG-based refinement if confidence is low.\n",
    "    \"\"\"\n",
    "    # --- 1. Initial Model Prediction ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare inputs\n",
    "        img_input = image_tensor.unsqueeze(0).to(DEVICE)\n",
    "        tokens = tokenizer(description, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        text_input = {k: v.to(DEVICE) for k, v in tokens.items()}\n",
    "        \n",
    "        # Get model output\n",
    "        logits = model(img_input, text_input)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        confidence, pred_idx = torch.max(probabilities, dim=1)\n",
    "        \n",
    "        initial_prediction = IDX_TO_CLASS[pred_idx.item()]\n",
    "        confidence_score = confidence.item()\n",
    "\n",
    "    print(f\"Initial Prediction: '{initial_prediction}' with confidence: {confidence_score:.2%}\")\n",
    "\n",
    "    final_prediction = initial_prediction\n",
    "\n",
    "    # --- 2. Check Confidence and Trigger RAG if Needed ---\n",
    "    if confidence_score < CONFIDENCE_THRESHOLD:\n",
    "        print(f\"Confidence is below {CONFIDENCE_THRESHOLD:.0%}. Triggering RAG refinement... ü§î\")\n",
    "        \n",
    "        # --- 3. Retrieve Questions from Knowledge Base ---\n",
    "        questions = kb.get_differential_questions(description)\n",
    "        \n",
    "        if not questions:\n",
    "            print(\"No relevant differential questions found in the KB.\")\n",
    "        else:\n",
    "            print(\"\\\\nAsking clarifying questions:\")\n",
    "            for q in questions:\n",
    "                print(f\"  - {q}\")\n",
    "            \n",
    "            # --- 4. Simulate User Answers ---\n",
    "            # In a real app, you would capture user input here.\n",
    "            # For this demonstration, we'll simulate the answers.\n",
    "            answers = {q: random.choice(['yes', 'no']) for q in questions}\n",
    "            print(\"\\\\nSimulated Answers:\")\n",
    "            for q, a in answers.items():\n",
    "                print(f\"  - Q: {q} -> A: {a}\")\n",
    "\n",
    "            # --- 5. Refine Prediction with Answers ---\n",
    "            refined_disease, scores = kb.refine_prediction(description, answers)\n",
    "            if refined_disease:\n",
    "                print(f\"\\\\nKB Refined Prediction: '{refined_disease}'\")\n",
    "                final_prediction = refined_disease\n",
    "            else:\n",
    "                print(\"\\\\nCould not refine prediction with the given answers.\")\n",
    "    else:\n",
    "        print(\"Confidence is high. Sticking with the initial prediction. ‚úÖ\")\n",
    "\n",
    "    print(f\"\\\\n--- Final Result for {os.path.basename(rel_path)} ---\")\n",
    "    print(f\"==> Final Diagnosis: {final_prediction}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T15:34:42.031799Z",
     "iopub.status.busy": "2025-10-09T15:34:42.031630Z",
     "iopub.status.idle": "2025-10-09T16:27:38.121222Z",
     "shell.execute_reply": "2025-10-09T16:27:38.120219Z",
     "shell.execute_reply.started": "2025-10-09T15:34:42.031787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Stage 1: Vision Pre-training ---\n",
      "Excluding 2100 images that have text descriptions.\n",
      "Found 5605 images for Stage-1 vision pre-training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84.2M/84.2M [00:00<00:00, 215MB/s] \n",
      "Epoch 1/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:31<00:00,  1.54it/s]\n",
      "Epoch 1/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:20<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.6409, Train Acc: 0.4110 | Val Loss: 1.3096, Val Acc: 0.4469\n",
      "‚úÖ Best model saved with Val Acc: 0.4469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:04<00:00,  2.19it/s]\n",
      "Epoch 2/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.1719, Train Acc: 0.5408 | Val Loss: 1.1351, Val Acc: 0.5433\n",
      "‚úÖ Best model saved with Val Acc: 0.5433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.09it/s]\n",
      "Epoch 3/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.9754, Train Acc: 0.6169 | Val Loss: 1.1493, Val Acc: 0.5468\n",
      "‚úÖ Best model saved with Val Acc: 0.5468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:06<00:00,  2.12it/s]\n",
      "Epoch 4/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.7999, Train Acc: 0.6804 | Val Loss: 1.0658, Val Acc: 0.5959\n",
      "‚úÖ Best model saved with Val Acc: 0.5959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.10it/s]\n",
      "Epoch 5/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.6882, Train Acc: 0.7212 | Val Loss: 1.1990, Val Acc: 0.6450\n",
      "‚úÖ Best model saved with Val Acc: 0.6450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.10it/s]\n",
      "Epoch 6/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.5583, Train Acc: 0.7743 | Val Loss: 1.0493, Val Acc: 0.6405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.10it/s]\n",
      "Epoch 7/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.4557, Train Acc: 0.8158 | Val Loss: 1.1289, Val Acc: 0.6878\n",
      "‚úÖ Best model saved with Val Acc: 0.6878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.09it/s]\n",
      "Epoch 8/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:15<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.3763, Train Acc: 0.8483 | Val Loss: 1.3715, Val Acc: 0.6762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:10<00:00,  2.01it/s]\n",
      "Epoch 9/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:15<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.2955, Train Acc: 0.8805 | Val Loss: 1.1992, Val Acc: 0.6851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:08<00:00,  2.05it/s]\n",
      "Epoch 10/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.2613, Train Acc: 0.8979 | Val Loss: 1.3246, Val Acc: 0.6922\n",
      "‚úÖ Best model saved with Val Acc: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.08it/s]\n",
      "Epoch 11/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.2077, Train Acc: 0.9159 | Val Loss: 1.3624, Val Acc: 0.7190\n",
      "‚úÖ Best model saved with Val Acc: 0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.08it/s]\n",
      "Epoch 12/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.1816, Train Acc: 0.9289 | Val Loss: 1.4955, Val Acc: 0.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.08it/s]\n",
      "Epoch 13/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.2742, Train Acc: 0.8967 | Val Loss: 1.2937, Val Acc: 0.7021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.08it/s]\n",
      "Epoch 14/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.1685, Train Acc: 0.9367 | Val Loss: 1.2807, Val Acc: 0.6967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.09it/s]\n",
      "Epoch 15/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.1627, Train Acc: 0.9405 | Val Loss: 1.5468, Val Acc: 0.7270\n",
      "‚úÖ Best model saved with Val Acc: 0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.10it/s]\n",
      "Epoch 16/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.1191, Train Acc: 0.9565 | Val Loss: 1.2893, Val Acc: 0.7065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.10it/s]\n",
      "Epoch 17/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.1408, Train Acc: 0.9529 | Val Loss: 1.5185, Val Acc: 0.6860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:07<00:00,  2.10it/s]\n",
      "Epoch 18/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.1134, Train Acc: 0.9538 | Val Loss: 1.4841, Val Acc: 0.7288\n",
      "‚úÖ Best model saved with Val Acc: 0.7288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:06<00:00,  2.11it/s]\n",
      "Epoch 19/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.0983, Train Acc: 0.9612 | Val Loss: 1.4545, Val Acc: 0.6940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:06<00:00,  2.11it/s]\n",
      "Epoch 20/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:14<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.1073, Train Acc: 0.9556 | Val Loss: 1.3999, Val Acc: 0.7154\n",
      "Training curves saved to /kaggle/working/training_curves_S1.png\n",
      "--- Stage 1 Complete ---\n",
      "\n",
      "--- Starting Stage 2: Multimodal Fine-tuning ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903dcfa09d894786b248d6f497357aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db96fed13fe04a1b9106f045bf079b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edce9392ba614590837fd5d679e51a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2030 multimodal samples for Stage-2 training.\n",
      "Found 2030 multimodal samples for Stage-2 training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Stage-1 vision weights from /kaggle/working/dino_finetuned.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a6f38d4fed45abba21d5176fc5fb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 110,072,064 || trainable%: 0.5359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [01:00<00:00,  1.68it/s]\n",
      "Epoch 1/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:10<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.7814, Train Acc: 0.3079 | Val Loss: 1.4390, Val Acc: 0.6700\n",
      "‚úÖ Best model saved with Val Acc: 0.6700\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.95it/s]\n",
      "Epoch 2/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.3758, Train Acc: 0.5425 | Val Loss: 1.0354, Val Acc: 0.6946\n",
      "‚úÖ Best model saved with Val Acc: 0.6946\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.97it/s]\n",
      "Epoch 3/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.0552, Train Acc: 0.6601 | Val Loss: 0.8108, Val Acc: 0.7611\n",
      "‚úÖ Best model saved with Val Acc: 0.7611\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.95it/s]\n",
      "Epoch 4/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.8286, Train Acc: 0.7395 | Val Loss: 0.5671, Val Acc: 0.8473\n",
      "‚úÖ Best model saved with Val Acc: 0.8473\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.94it/s]\n",
      "Epoch 5/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.6195, Train Acc: 0.8116 | Val Loss: 0.4231, Val Acc: 0.8744\n",
      "‚úÖ Best model saved with Val Acc: 0.8744\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.97it/s]\n",
      "Epoch 6/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.4896, Train Acc: 0.8461 | Val Loss: 0.2998, Val Acc: 0.9163\n",
      "‚úÖ Best model saved with Val Acc: 0.9163\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.97it/s]\n",
      "Epoch 7/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.3501, Train Acc: 0.8996 | Val Loss: 0.2204, Val Acc: 0.9458\n",
      "‚úÖ Best model saved with Val Acc: 0.9458\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.96it/s]\n",
      "Epoch 8/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.2754, Train Acc: 0.9249 | Val Loss: 0.1794, Val Acc: 0.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.97it/s]\n",
      "Epoch 9/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.2250, Train Acc: 0.9347 | Val Loss: 0.1387, Val Acc: 0.9581\n",
      "‚úÖ Best model saved with Val Acc: 0.9581\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.94it/s]\n",
      "Epoch 10/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.1709, Train Acc: 0.9544 | Val Loss: 0.1186, Val Acc: 0.9631\n",
      "‚úÖ Best model saved with Val Acc: 0.9631\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.94it/s]\n",
      "Epoch 11/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.1721, Train Acc: 0.9514 | Val Loss: 0.1070, Val Acc: 0.9704\n",
      "‚úÖ Best model saved with Val Acc: 0.9704\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.95it/s]\n",
      "Epoch 12/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.1266, Train Acc: 0.9674 | Val Loss: 0.0931, Val Acc: 0.9729\n",
      "‚úÖ Best model saved with Val Acc: 0.9729\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.97it/s]\n",
      "Epoch 13/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.1038, Train Acc: 0.9717 | Val Loss: 0.0896, Val Acc: 0.9778\n",
      "‚úÖ Best model saved with Val Acc: 0.9778\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.96it/s]\n",
      "Epoch 14/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.0857, Train Acc: 0.9766 | Val Loss: 0.0906, Val Acc: 0.9754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.96it/s]\n",
      "Epoch 15/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.0745, Train Acc: 0.9778 | Val Loss: 0.0883, Val Acc: 0.9803\n",
      "‚úÖ Best model saved with Val Acc: 0.9803\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.97it/s]\n",
      "Epoch 16/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.0721, Train Acc: 0.9815 | Val Loss: 0.0844, Val Acc: 0.9828\n",
      "‚úÖ Best model saved with Val Acc: 0.9828\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.95it/s]\n",
      "Epoch 17/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.0618, Train Acc: 0.9834 | Val Loss: 0.0779, Val Acc: 0.9828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.95it/s]\n",
      "Epoch 18/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.0480, Train Acc: 0.9895 | Val Loss: 0.0623, Val Acc: 0.9828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.95it/s]\n",
      "Epoch 19/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.0465, Train Acc: 0.9865 | Val Loss: 0.0513, Val Acc: 0.9852\n",
      "‚úÖ Best model saved with Val Acc: 0.9852\n",
      "Confusion matrix saved to best_val_conf_matrix_S2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.95it/s]\n",
      "Epoch 20/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.0423, Train Acc: 0.9901 | Val Loss: 0.0715, Val Acc: 0.9803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.94it/s]\n",
      "Epoch 21/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.0427, Train Acc: 0.9883 | Val Loss: 0.0872, Val Acc: 0.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:52<00:00,  1.96it/s]\n",
      "Epoch 22/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.0252, Train Acc: 0.9945 | Val Loss: 0.0697, Val Acc: 0.9803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.98it/s]\n",
      "Epoch 23/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.0286, Train Acc: 0.9926 | Val Loss: 0.0668, Val Acc: 0.9828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:51<00:00,  1.97it/s]\n",
      "Epoch 24/30 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 0.0335, Train Acc: 0.9920 | Val Loss: 0.0633, Val Acc: 0.9828\n",
      "‚èπÔ∏è Early stopping triggered after 24 epochs.\n",
      "Training curves saved to /kaggle/working/training_curves_S2.png\n",
      "--- Stage 2 Complete ---\n",
      "\n",
      "--- Preparing for RAG Demonstration ---\n",
      "Initializing Knowledge Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing KB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 59.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base indexed with 7 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 110,072,064 || trainable%: 0.5359\n",
      "\n",
      "‚úÖ Loaded best model from /kaggle/working/magevpro_best.pth for RAG demo.\n",
      "\n",
      "üîç Running RAG inference demonstration on 5 validation samples...\n",
      "Found 2100 multimodal samples for Stage-2 training.\n",
      "\n",
      "--- Sample 1 ---\n",
      "Image Path: Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions/basal-cell-carcinoma-vulva-1.jpg\n",
      "True Label: bcc\n",
      "Description: This is a single, red bump or lump. It has a shiny, see-through quality to its edge, and I can see tiny blood vessels on it. The middle has some whitish scaling or scabbing. Since it's on a sun-exposed area, it's a primary concern.\n",
      "Initial Prediction: 'bcc' with confidence: 100.00%\n",
      "Confidence is high. Sticking with the initial prediction. ‚úÖ\n",
      "\\n--- Final Result for basal-cell-carcinoma-vulva-1.jpg ---\n",
      "==> Final Diagnosis: bcc\n",
      "--------------------\n",
      "\n",
      "--- Sample 2 ---\n",
      "Image Path: Herpes HPV and other STDs Photos/herpes-type-2-recurrent-42.jpg\n",
      "True Label: stds\n",
      "Description: I have a cluster of shallow, open sores on a red, swollen patch of skin in my genital area. They look like blisters that have popped. They're not crusty yet.\n",
      "Initial Prediction: 'stds' with confidence: 99.97%\n",
      "Confidence is high. Sticking with the initial prediction. ‚úÖ\n",
      "\\n--- Final Result for herpes-type-2-recurrent-42.jpg ---\n",
      "==> Final Diagnosis: stds\n",
      "--------------------\n",
      "\n",
      "--- Sample 3 ---\n",
      "Image Path: Herpes HPV and other STDs Photos/genital-warts-65.jpg\n",
      "True Label: stds\n",
      "Description: I have a single, pinkish, warty bump in my genital area. It feels firm and has a rough surface. The skin around it is a little bit red.\n",
      "Initial Prediction: 'stds' with confidence: 99.99%\n",
      "Confidence is high. Sticking with the initial prediction. ‚úÖ\n",
      "\\n--- Final Result for genital-warts-65.jpg ---\n",
      "==> Final Diagnosis: stds\n",
      "--------------------\n",
      "\n",
      "--- Sample 4 ---\n",
      "Image Path: Eczema Photos/lichen-simplex-chronicus-184.jpg\n",
      "True Label: eczema\n",
      "Description: I have a red rash on the side of my ankle and foot with blurry edges. The skin is dry, flaky, and looks thick and leathery. It's not weeping or crusty.\n",
      "Initial Prediction: 'eczema' with confidence: 99.98%\n",
      "Confidence is high. Sticking with the initial prediction. ‚úÖ\n",
      "\\n--- Final Result for lichen-simplex-chronicus-184.jpg ---\n",
      "==> Final Diagnosis: eczema\n",
      "--------------------\n",
      "\n",
      "--- Sample 5 ---\n",
      "Image Path: Tinea Ringworm Candidiasis and other Fungal Infections/erosio-interdigitalis-blastomycetica-37.jpg\n",
      "True Label: fungal\n",
      "Description: The skin between my toes is bright red, flaky, and looks wet and raw. It's very irritated from being in a moist area.\n",
      "Initial Prediction: 'fungal' with confidence: 100.00%\n",
      "Confidence is high. Sticking with the initial prediction. ‚úÖ\n",
      "\\n--- Final Result for erosio-interdigitalis-blastomycetica-37.jpg ---\n",
      "==> Final Diagnosis: fungal\n",
      "--------------------\n",
      "\n",
      "==================================================\n",
      "--- STARTING FINAL EVALUATION ON HOLD-OUT TEST SET ---\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_test_set_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1810583061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Step 1: Physically create the test set directory from the test JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     create_test_set_from_json(\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0msource_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSOURCE_BASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdest_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_test_set_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "# ================== MAIN EXECUTION ===================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Run Training Stages ---\n",
    "    run_stage1()\n",
    "    run_stage2()\n",
    "\n",
    "    # --- Setup for RAG Demonstration ---\n",
    "    print(\"\\n--- Preparing for RAG Demonstration ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "    text_encoder_for_kb = BertModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "    kb = KnowledgeBase(KB_JSON, tokenizer, text_encoder_for_kb)\n",
    "\n",
    "    # Load the best model from Stage 2 for the demo\n",
    "    final_model = MAGEVPro().to(DEVICE)\n",
    "    final_model.load_state_dict(torch.load(MAGEVPRO_CHECKPOINT_PATH, map_location=DEVICE))\n",
    "    print(f\"\\n‚úÖ Loaded best model from {MAGEVPRO_CHECKPOINT_PATH} for RAG demo.\")\n",
    "    \n",
    "    # --- RAG Demonstration on Validation Samples ---\n",
    "    print(f\"\\nüîç Running RAG inference demonstration on {NUM_INFERENCE_SAMPLES} validation samples...\")\n",
    "    val_tf = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)])\n",
    "    val_ds_for_demo = DermDataset(IMG_DIR, TEXT_JSON, tokenizer, val_tf)\n",
    "    labels = [s[2] for s in val_ds_for_demo.samples]\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE_S2, random_state=42)\n",
    "    _, val_idx = next(sss.split(np.zeros(len(labels)), labels))\n",
    "    val_subset = Subset(val_ds_for_demo, val_idx)\n",
    "\n",
    "    for i in range(min(NUM_INFERENCE_SAMPLES, len(val_subset))):\n",
    "        image_tensor, _, true_label_idx, description, rel_path = val_subset[i]\n",
    "        \n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Image Path: {rel_path}\")\n",
    "        print(f\"True Label: {IDX_TO_CLASS[true_label_idx]}\")\n",
    "        print(f\"Description: {description}\")\n",
    "        \n",
    "        infer_with_rag(final_model, kb, image_tensor, description, tokenizer, rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T16:28:26.557364Z",
     "iopub.status.busy": "2025-10-09T16:28:26.556640Z",
     "iopub.status.idle": "2025-10-09T16:28:26.997729Z",
     "shell.execute_reply": "2025-10-09T16:28:26.997164Z",
     "shell.execute_reply.started": "2025-10-09T16:28:26.557339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created empty directory: /kaggle/working/test_images\n",
      "\n",
      "Reading 70 entries from JSON and copying files...\n",
      "\n",
      "--- Test Set Creation Summary ---\n",
      "‚úÖ Successfully copied 70 images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# ================== CONFIGURATION ===================\n",
    "# The source directory where the original training images are located.\n",
    "SOURCE_BASE_DIR = \"/kaggle/input/dermnet/train\"\n",
    "\n",
    "# The JSON file that lists which images to include in the test set.\n",
    "TEST_TEXT_JSON = \"/kaggle/input/test-2-final/test2_final.json\" \n",
    "\n",
    "# The destination directory where the new test set will be created.\n",
    "TEST_DIR = \"/kaggle/working/test_images\"\n",
    "\n",
    "# ================== SCRIPT ===================\n",
    "\n",
    "def create_test_set_from_json(source_dir, dest_dir, json_path):\n",
    "    \"\"\"\n",
    "    Creates a persistent test dataset by copying files listed in a JSON file.\n",
    "    \"\"\"\n",
    "    # 1. Clean up and create the destination directory for a fresh start.\n",
    "    if os.path.exists(dest_dir):\n",
    "        print(f\"Directory '{dest_dir}' already exists. Removing it...\")\n",
    "        shutil.rmtree(dest_dir)\n",
    "    os.makedirs(dest_dir)\n",
    "    print(f\"Successfully created empty directory: {dest_dir}\")\n",
    "\n",
    "    # 2. Load the list of test images from the JSON file.\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            images_to_copy = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"üõë Error: The JSON file was not found at '{json_path}'\")\n",
    "        return\n",
    "\n",
    "    # 3. Iterate, create subfolders, and copy images.\n",
    "    total_copied = 0\n",
    "    total_missing = 0\n",
    "    print(f\"\\nReading {len(images_to_copy)} entries from JSON and copying files...\")\n",
    "\n",
    "    for relative_path, info in images_to_copy.items():\n",
    "        class_name = info.get(\"class\")\n",
    "        if not class_name:\n",
    "            print(f\"  [Skipping] Missing 'class' for entry: {relative_path}\")\n",
    "            continue\n",
    "\n",
    "        # Create the class-specific subfolder (e.g., /kaggle/working/test_images/acne)\n",
    "        class_dir = os.path.join(dest_dir, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "        # Define source and destination paths for the image\n",
    "        source_path = os.path.join(source_dir, relative_path)\n",
    "        destination_path = os.path.join(class_dir, os.path.basename(relative_path))\n",
    "\n",
    "        # Copy the file if it exists\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy2(source_path, destination_path)\n",
    "            total_copied += 1\n",
    "        else:\n",
    "            print(f\"  [Warning] Source file not found and was skipped: {source_path}\")\n",
    "            total_missing += 1\n",
    "\n",
    "    # 4. Print a final summary.\n",
    "    print(\"\\n--- Test Set Creation Summary ---\")\n",
    "    print(f\"‚úÖ Successfully copied {total_copied} images.\")\n",
    "    if total_missing > 0:\n",
    "        print(f\"‚ö†Ô∏è Could not find {total_missing} source images.\")\n",
    "\n",
    "# --- Execute the function ---\n",
    "create_test_set_from_json(\n",
    "    source_dir=SOURCE_BASE_DIR,\n",
    "    dest_dir=TEST_DIR,\n",
    "    json_path=TEST_TEXT_JSON\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T16:28:34.069811Z",
     "iopub.status.busy": "2025-10-09T16:28:34.069572Z",
     "iopub.status.idle": "2025-10-09T16:28:40.496184Z",
     "shell.execute_reply": "2025-10-09T16:28:40.495408Z",
     "shell.execute_reply.started": "2025-10-09T16:28:34.069796Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged 70 images with descriptions for evaluation.\n",
      "\n",
      "--- Initializing for Stage-1 (Vision-Only) Evaluation ---\n",
      "‚úÖ Loaded Stage-1 model from /kaggle/working/dino_finetuned.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stage-1 Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---=== Stage-1 Vision-Only Test Set Metrics ===---\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                acne     0.9000    0.9000    0.9000        10\n",
      "           psoriasis     0.6154    0.8000    0.6957        10\n",
      "              eczema     0.7500    0.6000    0.6667        10\n",
      "                stds     1.0000    0.4000    0.5714        10\n",
      "              fungal     0.8889    0.8000    0.8421        10\n",
      "                 bcc     0.4286    0.6000    0.5000        10\n",
      "seborrheic_keratosis     0.7500    0.9000    0.8182        10\n",
      "\n",
      "            accuracy                         0.7143        70\n",
      "           macro avg     0.7618    0.7143    0.7134        70\n",
      "        weighted avg     0.7618    0.7143    0.7134        70\n",
      "\n",
      "‚úÖ Confusion matrix saved to stage1_test_confusion_matrix.png\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Initializing for Stage-2 (Multimodal) Evaluation ---\n",
      "‚úÖ Loaded Stage-2 model from /kaggle/working/magevpro_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stage-2 Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---=== Stage-2 Multimodal Test Set Metrics ===---\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                acne     1.0000    1.0000    1.0000        10\n",
      "           psoriasis     0.9091    1.0000    0.9524        10\n",
      "              eczema     1.0000    0.8000    0.8889        10\n",
      "                stds     1.0000    1.0000    1.0000        10\n",
      "              fungal     0.9000    0.9000    0.9000        10\n",
      "                 bcc     0.9091    1.0000    0.9524        10\n",
      "seborrheic_keratosis     1.0000    1.0000    1.0000        10\n",
      "\n",
      "            accuracy                         0.9571        70\n",
      "           macro avg     0.9597    0.9571    0.9562        70\n",
      "        weighted avg     0.9597    0.9571    0.9562        70\n",
      "\n",
      "‚úÖ Confusion matrix saved to stage2_test_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn for metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Transformers & PEFT for the language model\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# ================== CONFIG ===================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Paths ---\n",
    "SOURCE_BASE_DIR = \"/kaggle/input/dermnet/train\"\n",
    "TEST_DIR = \"/kaggle/working/test_images\"\n",
    "TEST_TEXT_JSON = \"/kaggle/input/test-2-final/test2_final.json\"\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# --- Checkpoint Paths ---\n",
    "DINO_CHECKPOINT_PATH = \"/kaggle/working/dino_finetuned.pth\" # <-- ADDED for Stage-1\n",
    "MAGEVPRO_CHECKPOINT_PATH = \"/kaggle/working/magevpro_best.pth\"\n",
    "\n",
    "\n",
    "# --- Class Mappings ---\n",
    "CLASS_MAPPINGS = {\n",
    "    'Acne and Rosacea Photos': 'acne',\n",
    "    'Psoriasis pictures Lichen Planus and related diseases': 'psoriasis',\n",
    "    'Eczema Photos': 'eczema',\n",
    "    'Herpes HPV and other STDs Photos': 'stds',\n",
    "    'Tinea Ringworm Candidiasis and other Fungal Infections': 'fungal',\n",
    "    'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions': 'bcc',\n",
    "    'Seborrheic Keratoses and other Benign Tumors': 'seborrheic_keratosis'\n",
    "}\n",
    "FINAL_CLASSES = list(CLASS_MAPPINGS.values())\n",
    "CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(FINAL_CLASSES)}\n",
    "SHORT_TO_FULL = {v: k for k, v in CLASS_MAPPINGS.items()}\n",
    "\n",
    "# ================== 1. DATA LOADING ===================\n",
    "def merge_images_with_descriptions(test_dir, json_path):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples: (image_path, description, class_label)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            text_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"üõë JSON file not found: {json_path}\")\n",
    "        return []\n",
    "\n",
    "    merged_samples = []\n",
    "    for short_class_name in os.listdir(test_dir):\n",
    "        class_path = os.path.join(test_dir, short_class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        full_class_name = SHORT_TO_FULL.get(short_class_name)\n",
    "        if not full_class_name:\n",
    "            continue\n",
    "\n",
    "        for img_file in os.listdir(class_path):\n",
    "            if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                json_key = os.path.join(full_class_name, img_file)\n",
    "                description = text_data.get(json_key, {}).get(\"description\", \"A skin lesion (default).\")\n",
    "                merged_samples.append((os.path.join(class_path, img_file), description, short_class_name))\n",
    "\n",
    "    print(f\"‚úÖ Merged {len(merged_samples)} images with descriptions for evaluation.\")\n",
    "    return merged_samples\n",
    "\n",
    "# ================== 2. PYTORCH DATASETS ===================\n",
    "class VisionTestDataset(Dataset):\n",
    "    \"\"\"A PyTorch Dataset for VISION-ONLY evaluation.\"\"\"\n",
    "    def __init__(self, paired_data, transform):\n",
    "        self.paired_data = paired_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paired_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, _, label_str = self.paired_data[idx]\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        label = CLASS_TO_IDX[label_str]\n",
    "        return image, label\n",
    "\n",
    "class PairedTestDataset(Dataset):\n",
    "    \"\"\"A PyTorch Dataset to handle the 'paired' multimodal data.\"\"\"\n",
    "    def __init__(self, paired_data, tokenizer, transform):\n",
    "        self.paired_data = paired_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paired_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, text, label_str = self.paired_data[idx]\n",
    "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
    "        tokens = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        tokens = {k: v.squeeze(0) for k, v in tokens.items()}\n",
    "        label = CLASS_TO_IDX[label_str]\n",
    "        return image, tokens, label\n",
    "\n",
    "# ================== MODEL DEFINITIONS ===================\n",
    "class DinoVisionClassifier(nn.Module):\n",
    "    \"\"\"Complete Stage-1 Vision-only Model with classifier head.\"\"\"\n",
    "    def __init__(self, num_classes=len(FINAL_CLASSES)):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14', trust_repo=True, verbose=False)\n",
    "        self.classifier_head = nn.Linear(384, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vision_encoder(x)\n",
    "        return self.classifier_head(features)\n",
    "\n",
    "class MAGEVPro(nn.Module):\n",
    "    \"\"\"Complete Stage-2 Multimodal Model.\"\"\"\n",
    "    def __init__(self, num_classes=len(FINAL_CLASSES)):\n",
    "        super().__init__()\n",
    "        # Note: DINOv2 is loaded within this class definition for simplicity\n",
    "        dino_base = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14', trust_repo=True, verbose=False)\n",
    "        self.vision_encoder = dino_base\n",
    "        \n",
    "        self.text_encoder = BertModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "        lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"query\", \"value\"], lora_dropout=0.1, bias=\"none\")\n",
    "        self.text_encoder = get_peft_model(self.text_encoder, lora_config)\n",
    "        \n",
    "        self.film_gamma = nn.Linear(768, 384)\n",
    "        self.film_beta = nn.Linear(768, 384)\n",
    "        self.mlp = nn.Sequential(nn.ReLU(), nn.Dropout(0.5), nn.Linear(384, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, num_classes))\n",
    "\n",
    "    def forward(self, image, text_inputs):\n",
    "        vision_feat = self.vision_encoder(image)\n",
    "        text_feat = self.text_encoder(**text_inputs).last_hidden_state[:, 0, :]\n",
    "        gamma, beta = self.film_gamma(text_feat), self.film_beta(text_feat)\n",
    "        fused = gamma * vision_feat + beta\n",
    "        return self.mlp(fused)\n",
    "\n",
    "# ================== HELPER FUNCTION ===================\n",
    "def plot_conf_matrix(y_true, y_pred, title=\"Confusion Matrix\", save_path=\"confusion_matrix.png\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(FINAL_CLASSES)))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=FINAL_CLASSES, yticklabels=FINAL_CLASSES)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(title)\n",
    "    plt.tight_layout(); plt.savefig(save_path); plt.close()\n",
    "    print(f\"‚úÖ Confusion matrix saved to {save_path}\")\n",
    "\n",
    "# ================== EVALUATION FUNCTIONS ===================\n",
    "def run_vision_only_evaluation(paired_data):\n",
    "    \"\"\"\n",
    "    Evaluates the Stage-1 Vision-Only model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Initializing for Stage-1 (Vision-Only) Evaluation ---\")\n",
    "    test_tf = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)])\n",
    "    \n",
    "    dataset = VisionTestDataset(paired_data, test_tf)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = DinoVisionClassifier().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(DINO_CHECKPOINT_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    print(f\"‚úÖ Loaded Stage-1 model from {DINO_CHECKPOINT_PATH}\")\n",
    "\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Running Stage-1 Evaluation\"):\n",
    "            outputs = model(images.to(DEVICE))\n",
    "            preds = outputs.argmax(1)\n",
    "            all_true.extend(labels.cpu().tolist())\n",
    "            all_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "    print(\"\\n\\n---=== Stage-1 Vision-Only Test Set Metrics ===---\")\n",
    "    print(classification_report(all_true, all_pred, target_names=FINAL_CLASSES, digits=4))\n",
    "    plot_conf_matrix(all_true, all_pred, \n",
    "                     title=\"Stage-1 Vision-Only Confusion Matrix\", \n",
    "                     save_path=\"stage1_test_confusion_matrix.png\")\n",
    "\n",
    "def run_multimodal_evaluation(paired_data):\n",
    "    \"\"\"\n",
    "    Evaluates the Stage-2 Multimodal model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Initializing for Stage-2 (Multimodal) Evaluation ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "    test_tf = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)])\n",
    "    \n",
    "    dataset = PairedTestDataset(paired_data, tokenizer, test_tf)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = MAGEVPro().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(MAGEVPRO_CHECKPOINT_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    print(f\"‚úÖ Loaded Stage-2 model from {MAGEVPRO_CHECKPOINT_PATH}\")\n",
    "\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in tqdm(loader, desc=\"Running Stage-2 Evaluation\"):\n",
    "            texts = {k: v.to(DEVICE) for k, v in texts.items()}\n",
    "            outputs = model(images.to(DEVICE), texts)\n",
    "            preds = outputs.argmax(1)\n",
    "            all_true.extend(labels.cpu().tolist())\n",
    "            all_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "    print(\"\\n\\n---=== Stage-2 Multimodal Test Set Metrics ===---\")\n",
    "    print(classification_report(all_true, all_pred, target_names=FINAL_CLASSES, digits=4))\n",
    "    plot_conf_matrix(all_true, all_pred,\n",
    "                     title=\"Stage-2 Multimodal Confusion Matrix\",\n",
    "                     save_path=\"stage2_test_confusion_matrix.png\")\n",
    "\n",
    "# ================== MAIN EXECUTION ===================\n",
    "if __name__ == \"__main__\":\n",
    "    # This data contains all the info we need for both evaluations\n",
    "    paired_dataset = merge_images_with_descriptions(TEST_DIR, TEST_TEXT_JSON)\n",
    "    \n",
    "    if paired_dataset:\n",
    "        # Run Stage-1 Vision-Only Evaluation\n",
    "        run_vision_only_evaluation(paired_dataset)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\") # Separator\n",
    "        \n",
    "        # Run Stage-2 Multimodal Evaluation\n",
    "        run_multimodal_evaluation(paired_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T16:43:39.887306Z",
     "iopub.status.busy": "2025-10-09T16:43:39.887024Z",
     "iopub.status.idle": "2025-10-09T16:44:09.947240Z",
     "shell.execute_reply": "2025-10-09T16:44:09.946506Z",
     "shell.execute_reply.started": "2025-10-09T16:43:39.887276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating zip archive: trained_models.zip\n",
      "  Adding: dino_finetuned.pth...\n",
      "  Adding: magevpro_best.pth...\n",
      "\n",
      "‚úÖ Zip file created successfully at: /kaggle/working/trained_models.zip\n",
      "\n",
      "Click the link below to download the zip file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/kaggle/working/trained_models.zip' target='_blank'>/kaggle/working/trained_models.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/trained_models.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from IPython.display import display, FileLink\n",
    "\n",
    "# --- Configuration ---\n",
    "# List of files you want to include in the zip archive\n",
    "files_to_zip = [\n",
    "    \"/kaggle/working/dino_finetuned.pth\",\n",
    "    \"/kaggle/working/magevpro_best.pth\"\n",
    "]\n",
    "\n",
    "# Name for the output zip file\n",
    "zip_file_name = \"trained_models.zip\"\n",
    "zip_file_path = f\"/kaggle/working/{zip_file_name}\"\n",
    "\n",
    "# --- Zipping Process ---\n",
    "print(f\"Creating zip archive: {zip_file_name}\")\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in files_to_zip:\n",
    "            if os.path.exists(file):\n",
    "                print(f\"  Adding: {os.path.basename(file)}...\")\n",
    "                zipf.write(file, arcname=os.path.basename(file))\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è Skipping, file not found: {file}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Zip file created successfully at: {zip_file_path}\")\n",
    "\n",
    "    # --- Generate Download Link for the Zip File ---\n",
    "    print(\"\\nClick the link below to download the zip file:\")\n",
    "    display(FileLink(zip_file_path))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nüõë An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 735911,
     "sourceId": 1276317,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8255653,
     "sourceId": 13037868,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8390359,
     "sourceId": 13241588,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8391791,
     "sourceId": 13243821,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
